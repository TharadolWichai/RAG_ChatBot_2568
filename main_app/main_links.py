# main_links.py - AstraDB Version ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö links_data.py ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (Enhanced with PyThaiNLP)
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain_community.retrievers import BM25Retriever
from langchain.prompts import PromptTemplate
from langchain.schema import BaseRetriever, Document
from langchain.callbacks.manager import CallbackManagerForRetrieverRun
import os
from dotenv import load_dotenv
from typing import List 
from astrapy import DataAPIClient

# PyThaiNLP imports for advanced Thai processing
try:
    from pythainlp import word_tokenize, pos_tag
    from pythainlp.corpus import thai_stopwords
    from pythainlp.util import normalize
    from rank_bm25 import BM25Okapi
    PYTHAINLP_AVAILABLE = True
    print("‚úÖ PyThaiNLP loaded successfully")
except ImportError:
    PYTHAINLP_AVAILABLE = False
    print("‚ö†Ô∏è PyThaiNLP not available - falling back to basic search")

load_dotenv()

# ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° embedding
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Initialize AstraDB client
token = os.getenv("ASTRA_DB_APPLICATION_TOKEN")
api_endpoint = os.getenv("ASTRA_DB_API_ENDPOINT")
keyspace = os.getenv("ASTRA_DB_KEYSPACE", "default_keyspace")

if not token or not api_endpoint:
    print("‚ùå Error: Missing AstraDB credentials in .env file")
    exit(1)

client = DataAPIClient(token=token)
database = client.get_database_by_api_endpoint(api_endpoint)

# Get single collection for links data
try:
    collection = database.get_collection("links_embedding")
    print(f"‚úÖ Connected to collection: links_embedding")
except Exception as e:
    print(f"‚ùå Error accessing collection: {e}")
    exit(1)

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Retriever ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AstraDB (Links Collection)
class LinksRetriever(BaseRetriever):
    def __init__(self, collection, embedding):
        super().__init__()
        self._collection = collection
        self._embedding = embedding
        self._bm25_retriever = None  # Will be initialized when first used
        self._documents_cache = None  # Cache documents for BM25
    
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        print(f"üîç Debug: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏î‡πâ‡∏ß‡∏¢ query: '{query}'")
        
        # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        if any(word in query.lower() for word in ["‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡∏ó‡∏∏‡∏Å‡∏≠‡∏±‡∏ô", "all", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"]):
            print("üéØ ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î - ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°")
            return self._get_comprehensive_search()
        
        # Try multiple search strategies
        print("üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ AstraDB hybrid search...")
        
        # Strategy 0: Thai Exact Search (for Thai queries)
        thai_results = []
        is_thai_query = any('\u0e00' <= char <= '\u0e7f' for char in query)  # Check if contains Thai characters
        
        if is_thai_query:
            print("üîç ‡πÄ‡∏£‡∏¥‡πà‡∏° Advanced Thai Search...")
            thai_results = self._advanced_thai_search(query)
            if thai_results:
                print(f"üéØ Advanced Thai Search: ‡∏û‡∏ö {len(thai_results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

        # Strategy 1: Text search first for exact matches
        print("üîç ‡πÄ‡∏£‡∏¥‡πà‡∏° Text Search...")
        text_results = self._text_search(query)
        
        # Strategy 2: Vector search
        print("üß† ‡πÄ‡∏£‡∏¥‡πà‡∏° Vector Search...")
        vector_results = self._vector_search(query)
        
        # Combine results with hybrid scoring and ranking
        all_documents = []
        seen_content = set()
        
        print("üîÑ ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°...")
        
        # Collect all unique documents with their scores
        candidate_docs = []
        
        # Add Thai advanced search results first (highest priority)
        if thai_results:
            for i, doc in enumerate(thai_results[:5]):  # Top 5 Thai matches
                if doc.page_content not in seen_content:
                    # Get Thai score (could be advanced or exact)
                    thai_score = doc.metadata.get("thai_advanced_score", 0.0) or doc.metadata.get("thai_exact_score", 0.0)
                    search_type = doc.metadata.get("search_type", "thai_unknown")
                    
                    # Give Thai matches very high combined score
                    if thai_score > 10:  # Very high score
                        doc.metadata["combined_score"] = 0.9 + min(thai_score * 0.01, 0.1)  # 0.9-1.0
                    else:
                        doc.metadata["combined_score"] = 0.7 + (thai_score * 0.05)  # 0.7-0.95
                    
                    doc.metadata["bm25_score"] = 0.0
                    doc.metadata["vector_score"] = 0.0
                    candidate_docs.append(doc)
                    seen_content.add(doc.page_content)
                    
                    search_type_display = "üáπüá≠ Advanced" if search_type == "thai_advanced" else "üéØ Exact"
                    print(f"‚ûï Thai {search_type_display} #{i+1}: {doc.metadata.get('link_text', 'Unknown')} (Thai: {thai_score:.2f}, Combined: {doc.metadata['combined_score']:.4f})")
        
        # Add text search results
        for i, doc in enumerate(text_results):
            if doc.page_content not in seen_content:
                bm25_score = doc.metadata.get("bm25_score", 0.0)
                doc.metadata["combined_score"] = self._calculate_hybrid_score(doc, bm25_score, 0.0, query)
                candidate_docs.append(doc)
                seen_content.add(doc.page_content)
                print(f"‚ûï Text Search #{i+1}: {doc.metadata.get('link_text', 'Unknown')} (BM25: {bm25_score:.4f})")
        
        # Add vector search results
        for i, doc in enumerate(vector_results):
            if doc.page_content not in seen_content:
                vector_score = doc.metadata.get("vector_score", 0.0)
                doc.metadata["combined_score"] = self._calculate_hybrid_score(doc, 0.0, vector_score, query)
                candidate_docs.append(doc)
                seen_content.add(doc.page_content)
                print(f"‚ûï Vector Search #{i+1}: {doc.metadata.get('link_text', 'Unknown')} (Vector: {vector_score:.4f})")
            else:
                # Update existing document with vector score
                for existing_doc in candidate_docs:
                    if existing_doc.page_content == doc.page_content:
                        vector_score = doc.metadata.get("vector_score", 0.0)
                        bm25_score = existing_doc.metadata.get("bm25_score", 0.0)
                        existing_doc.metadata["vector_score"] = vector_score
                        existing_doc.metadata["combined_score"] = self._calculate_hybrid_score(existing_doc, bm25_score, vector_score, query)
                        print(f"üîÑ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {existing_doc.metadata.get('link_text', 'Unknown')} (BM25: {bm25_score:.4f}, Vector: {vector_score:.4f})")
                        break
        
        # Sort by combined score (descending)
        candidate_docs.sort(key=lambda doc: doc.metadata.get("combined_score", 0), reverse=True)
        
        # Take top results
        all_documents = candidate_docs
        
        # Show final ranking with scores
        print("\nüèÜ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°):")
        print("-" * 60)
        for i, doc in enumerate(all_documents[:5], 1):
            combined_score = doc.metadata.get("combined_score", 0.0)
            bm25_score = doc.metadata.get("bm25_score", 0.0)
            vector_score = doc.metadata.get("vector_score", 0.0)
            link_text = doc.metadata.get('link_text', 'Unknown')
            
            print(f"#{i}: {link_text}")
            print(f"    üéØ Combined: {combined_score:.4f} | üìù BM25: {bm25_score:.4f} | üß† Vector: {vector_score:.4f}")
            print("-" * 40)
        
        print(f"üìä ‡∏™‡∏£‡∏∏‡∏õ: Text={len(text_results)}, Vector={len(vector_results)}, ‡∏£‡∏ß‡∏°={len(all_documents)} (unique)")
        
        return all_documents[:10]  # Return top 10 results
    
    def _get_comprehensive_search(self) -> List[Document]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å collection"""
        print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏à‡∏≤‡∏Å links_embedding collection...")
        
        all_documents = []
        
        try:
            print("üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å links_embedding collection...")
            results = self._collection.find({}, limit=50)  # Get up to 50 links
            
            for result in results:
                doc = Document(
                    page_content=result.get("content", ""),
                    metadata=result.get("metadata", {})
                )
                all_documents.append(doc)
            
            print(f"üìä ‡∏à‡∏≤‡∏Å links_embedding: {len(all_documents)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            
        except Exception as e:
            print(f"‚ùå Error in comprehensive search: {e}")
        
        print(f"üéØ ‡∏û‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏£‡∏ß‡∏°: {len(all_documents)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        return all_documents
    
    def _vector_search(self, query: str) -> List[Document]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö vector search ‡∏à‡∏≤‡∏Å collection ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á"""
        all_documents = []
        
        try:
            print(f"üß† Vector Search: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö query: '{query}'")
            
            # Generate query embedding
            query_vector = self._embedding.embed_query(query)
            print(f"üìä Vector Search: ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡πÅ‡∏•‡πâ‡∏ß (dimension: {len(query_vector)})")
            
            # Perform vector search with similarity scores
            results = self._collection.find(
                {},
                sort={"$vector": query_vector},
                limit=10,  # Top 10 semantic matches
                include_similarity=True  # Include cosine similarity scores
            )
            
            for result in results:
                # Get similarity score (cosine similarity from AstraDB)
                similarity_score = result.get("$similarity", 0.0)
                
                # Create metadata with score
                metadata = result.get("metadata", {}).copy()
                metadata["vector_score"] = similarity_score
                metadata["search_type"] = "vector"
                
                doc = Document(
                    page_content=result.get("content", ""),
                    metadata=metadata
                )
                all_documents.append(doc)
            
            print(f"üß† Vector search: ‡∏û‡∏ö {len(all_documents)} documents ‡∏à‡∏≤‡∏Å semantic similarity")
            
            # Show top matches with scores
            if all_documents:
                print("   üèÜ Top Vector Matches:")
                for i, doc in enumerate(all_documents[:3]):
                    score = doc.metadata.get("vector_score", 0.0)
                    link_text = doc.metadata.get('link_text', 'Unknown')
                    print(f"   #{i+1}: {link_text} (score: {score:.4f})")
            
            return all_documents
            
        except Exception as e:
            print(f"‚ùå Error in vector search: {e}")
            return []

    def _ensure_bm25_initialized(self):
        """Initialize BM25 retriever with Thai tokenization support"""
        if self._bm25_retriever is None:
            print("üîß Initializing Enhanced BM25 retriever with Thai support...")
            try:
                # Get all documents from collection for BM25
                results = self._collection.find({}, limit=100)
                documents = []
                
                for result in results:
                    doc = Document(
                        page_content=result.get("content", ""),
                        metadata=result.get("metadata", {})
                    )
                    documents.append(doc)
                
                self._documents_cache = documents
                print(f"üìö Loaded {len(documents)} documents for BM25")
                
                # Create Enhanced BM25 with Thai tokenization
                if documents and PYTHAINLP_AVAILABLE:
                    self._create_thai_bm25(documents)
                    print("‚úÖ Enhanced Thai BM25 initialized successfully")
                elif documents:
                    # Fallback to standard BM25
                    self._bm25_retriever = BM25Retriever.from_documents(documents)
                    self._bm25_retriever.k = 10
                    print("‚úÖ Standard BM25 initialized successfully (PyThaiNLP not available)")
                else:
                    print("‚ö†Ô∏è No documents found for BM25 initialization")
                    
            except Exception as e:
                print(f"‚ùå Error initializing BM25: {e}")
                self._bm25_retriever = None
    
    def _create_thai_bm25(self, documents: List[Document]):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á BM25 ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Thai tokenization"""
        try:
            # Tokenize all documents with PyThaiNLP
            tokenized_docs = []
            
            for doc in documents:
                content = doc.page_content
                
                # Normalize and tokenize
                normalized = normalize(content)
                tokens = word_tokenize(normalized, engine='newmm')
                
                # Filter meaningful tokens
                stopwords = thai_stopwords()
                filtered_tokens = []
                
                for token in tokens:
                    token_clean = token.strip()
                    if (len(token_clean) > 1 and 
                        token_clean not in stopwords and
                        not token_clean.isspace()):
                        filtered_tokens.append(token_clean.lower())
                
                tokenized_docs.append(filtered_tokens)
            
            # Create BM25 with tokenized documents
            self._thai_bm25 = BM25Okapi(tokenized_docs)
            self._thai_bm25_docs = documents  # Keep reference to original docs
            
            print(f"üáπüá≠ Thai BM25 created with {len(tokenized_docs)} tokenized documents")
            
        except Exception as e:
            print(f"‚ùå Error creating Thai BM25: {e}")
            # Fallback to standard BM25
            self._bm25_retriever = BM25Retriever.from_documents(documents)
            self._bm25_retriever.k = 10

    def _preprocess_query_for_bm25(self, query: str) -> str:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ BM25"""
        import re
        
        # Remove common prefixes but keep the core keywords
        processed = query.replace("‡∏Ç‡∏≠", "").replace("‡∏•‡∏¥‡∏á‡∏Å‡πå", "").replace("‡∏•‡∏¥‡∏á‡∏Ñ‡πå", "").replace("‡∏£‡∏∞‡∏ö‡∏ö", "").strip()
        
        # ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
        if "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°" in query:
            processed = "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á ‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°"
        elif "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö" in query:
            processed = "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á ‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£"
        elif "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á" in query:
            processed = "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á"
        
        print(f"üî§ Query preprocessing: '{query}' ‚Üí '{processed}'")
        return processed if processed else query

    def _text_search(self, query: str) -> List[Document]:
        """Enhanced BM25 text search with Thai tokenization support"""
        try:
            # Ensure BM25 is initialized
            self._ensure_bm25_initialized()
            
            # Check if Thai BM25 is available
            if hasattr(self, '_thai_bm25') and PYTHAINLP_AVAILABLE:
                return self._thai_bm25_search(query)
            elif self._bm25_retriever is not None:
                return self._standard_bm25_search(query)
            else:
                print("‚ö†Ô∏è BM25 not available, falling back to keyword search")
                return self._fallback_keyword_search(query)
                
        except Exception as e:
            print(f"‚ùå Error in text search: {e}")
            return self._fallback_keyword_search(query)
    
    def _thai_bm25_search(self, query: str) -> List[Document]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Thai BM25 ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ PyThaiNLP tokenization"""
        try:
            print(f"üáπüá≠ Thai BM25 Search: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ query: '{query}'")
            
            # Tokenize query with PyThaiNLP
            normalized_query = normalize(query)
            query_tokens = word_tokenize(normalized_query, engine='newmm')
            
            # Filter meaningful tokens
            stopwords = thai_stopwords()
            filtered_tokens = []
            
            for token in query_tokens:
                token_clean = token.strip().lower()
                if (len(token_clean) > 1 and 
                    token_clean not in stopwords and
                    not token_clean.isspace()):
                    filtered_tokens.append(token_clean)
            
            if not filtered_tokens:
                filtered_tokens = [token.lower() for token in query_tokens if len(token.strip()) > 1]
            
            print(f"   üî§ Query tokens: {query_tokens}")
            print(f"   üéØ Filtered tokens: {filtered_tokens}")
            
            # Get BM25 scores
            bm25_scores = self._thai_bm25.get_scores(filtered_tokens)
            
            # Create results with scores
            results = []
            for i, score in enumerate(bm25_scores):
                if score > 0:  # Only include documents with positive scores
                    doc = self._thai_bm25_docs[i]
                    
                    # Add BM25 score to metadata
                    doc.metadata = doc.metadata.copy()  # Avoid modifying original
                    doc.metadata["bm25_score"] = score
                    doc.metadata["search_type"] = "thai_bm25"
                    doc.metadata["query_tokens"] = filtered_tokens
                    
                    results.append(doc)
            
            # Sort by BM25 score (descending)
            results.sort(key=lambda doc: doc.metadata.get("bm25_score", 0), reverse=True)
            
            print(f"üìù Thai BM25 search: ‡∏û‡∏ö {len(results)} documents")
            
            # Show top matches
            if results:
                print("   üèÜ Top Thai BM25 Matches:")
                for i, doc in enumerate(results[:3]):
                    score = doc.metadata.get("bm25_score", 0.0)
                    link_text = doc.metadata.get('link_text', 'Unknown')
                    print(f"   #{i+1}: {link_text} (Thai BM25: {score:.4f})")
            
            return results[:10]  # Return top 10
            
        except Exception as e:
            print(f"‚ùå Error in Thai BM25 search: {e}")
            return self._standard_bm25_search(query)
    
    def _standard_bm25_search(self, query: str) -> List[Document]:
        """Standard BM25 search (fallback when PyThaiNLP not available)"""
        try:
            print(f"üîç Standard BM25 Search: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ query: '{query}'")
            
            # Try multiple query variations for better matching
            query_variations = [
                query,  # Original query
                self._preprocess_query_for_bm25(query),  # Preprocessed
                query.replace("‡∏Ç‡∏≠", "").replace("‡∏•‡∏¥‡∏á‡∏Å‡πå", "").replace("‡∏•‡∏¥‡∏á‡∏Ñ‡πå", "").strip(),  # Clean version
            ]
            
            # Add word-by-word variations for compound queries
            if "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°" in query:
                query_variations.extend(["‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á", "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "‡∏à‡∏≠‡∏á ‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°"])
            if "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö" in query or "‡∏à‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö" in query:
                query_variations.extend(["‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á", "‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£", "‡∏à‡∏≠‡∏á ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£"])
            
            # Remove duplicates while preserving order
            unique_variations = []
            for q in query_variations:
                if q and q not in unique_variations:
                    unique_variations.append(q)
            
            all_bm25_results = []
            seen_content = set()
            
            for i, q_variant in enumerate(unique_variations):
                print(f"üîç BM25 Variant #{i+1}: '{q_variant}'")
                try:
                    variant_results = self._bm25_retriever.get_relevant_documents(q_variant)
                    
                    # Calculate BM25 scores for this variant
                    scored_results = self._calculate_bm25_scores(variant_results, q_variant)
                    
                    # Add unique results with scores
                    for doc, bm25_score in scored_results:
                        if doc.page_content not in seen_content:
                            # Add enhanced BM25 score to metadata
                            doc.metadata = doc.metadata.copy()
                            doc.metadata["bm25_score"] = bm25_score
                            doc.metadata["search_type"] = "standard_bm25"
                            doc.metadata["query_variant"] = q_variant
                            
                            all_bm25_results.append(doc)
                            seen_content.add(doc.page_content)
                    
                    print(f"   Found {len(variant_results)} docs (unique so far: {len(all_bm25_results)})")
                    
                except Exception as e:
                    print(f"   Error with variant '{q_variant}': {e}")
            
            # Sort by BM25 score (descending)
            all_bm25_results.sort(key=lambda doc: doc.metadata.get("bm25_score", 0), reverse=True)
            
            print(f"üìù Standard BM25 search: ‡∏û‡∏ö {len(all_bm25_results)} documents ‡∏£‡∏ß‡∏°")
            
            # Show top matches with scores
            if all_bm25_results:
                print("   üèÜ Top BM25 Matches:")
                for i, doc in enumerate(all_bm25_results[:3]):
                    score = doc.metadata.get("bm25_score", 0.0)
                    link_text = doc.metadata.get('link_text', 'Unknown')
                    print(f"   #{i+1}: {link_text} (BM25: {score:.4f})")
            
            return all_bm25_results[:10]  # Limit to top 10
            
        except Exception as e:
            print(f"‚ùå Error in standard BM25 search: {e}")
            return self._fallback_keyword_search(query)
    
    def _calculate_bm25_scores(self, documents: List[Document], query: str) -> List[tuple]:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô BM25 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö documents"""
        try:
            # Simple BM25-like scoring based on term frequency and document length
            import re
            from collections import Counter
            
            query_terms = re.findall(r'\w+', query.lower())
            if not query_terms:
                return [(doc, 0.0) for doc in documents]
            
            scored_docs = []
            
            for doc in documents:
                content_lower = doc.page_content.lower()
                content_terms = re.findall(r'\w+', content_lower)
                
                if not content_terms:
                    scored_docs.append((doc, 0.0))
                    continue
                
                # Calculate term frequency
                term_freq = Counter(content_terms)
                doc_length = len(content_terms)
                
                # Simple BM25-like score
                score = 0.0
                for term in query_terms:
                    if term in term_freq:
                        tf = term_freq[term]
                        # Simplified BM25 formula (without IDF calculation)
                        score += (tf * 2.2) / (tf + 1.2 * (0.25 + 0.75 * doc_length / 50))
                
                # Boost score for exact matches in metadata
                link_text = doc.metadata.get('link_text', '').lower()
                keywords = doc.metadata.get('keywords', [])
                
                for term in query_terms:
                    if term in link_text:
                        score += 2.0  # Boost for title match
                    for keyword in keywords:
                        if term in keyword.lower():
                            score += 1.0  # Boost for keyword match
                
                scored_docs.append((doc, score))
            
            return scored_docs
            
        except Exception as e:
            print(f"‚ùå Error calculating BM25 scores: {e}")
            return [(doc, 0.0) for doc in documents]
    
    def _calculate_hybrid_score(self, doc: Document, bm25_score: float, vector_score: float, query: str) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å BM25 ‡πÅ‡∏•‡∏∞ Vector similarity"""
        try:
            # Weights for different components
            bm25_weight = 0.4      # 40% for text matching
            vector_weight = 0.4    # 40% for semantic similarity
            bonus_weight = 0.2     # 20% for exact matches and metadata
            
            # Base scores (normalized)
            normalized_bm25 = min(bm25_score / 10.0, 1.0)  # Normalize BM25 to 0-1
            normalized_vector = vector_score  # Vector score is already 0-1
            
            # Calculate bonus score for exact matches
            bonus_score = 0.0
            query_lower = query.lower()
            link_text = doc.metadata.get('link_text', '').lower()
            keywords = doc.metadata.get('keywords', [])
            
            # Exact title match bonus
            if any(word in link_text for word in query_lower.split() if len(word) >= 2):
                bonus_score += 0.5
            
            # Keyword match bonus
            for keyword in keywords:
                if any(word in keyword.lower() for word in query_lower.split() if len(word) >= 2):
                    bonus_score += 0.3
                    break
            
            # Service type bonus (for specific service queries) - Enhanced
            service_bonuses = {
                "‡∏à‡∏≠‡∏á": 0.5 if "‡∏à‡∏≠‡∏á" in link_text else 0,  # Increased bonus for booking
                "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°": 0.5 if "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°" in link_text else 0,  # Increased bonus for meeting room
                "‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö": 0.3 if ("‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö" in link_text or "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£" in link_text) else 0,
                "‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î": 0.3 if ("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î" in link_text or "upload" in link_text.lower()) else 0,
                "reservation": 0.4 if "reservation" in link_text.lower() else 0,  # Added reservation bonus
            }
            
            for service_term, bonus in service_bonuses.items():
                if service_term in query_lower:
                    bonus_score += bonus
            
            # Special exact match bonus for common queries
            exact_match_bonuses = {
                "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°": 1.0 if ("‡∏à‡∏≠‡∏á" in query_lower and "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°" in query_lower and "‡∏à‡∏≠‡∏á" in link_text and "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°" in link_text) else 0,
                "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö": 1.0 if ("‡∏à‡∏≠‡∏á" in query_lower and ("‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö" in query_lower or "‡πÅ‡∏•‡πá‡∏ö" in query_lower) and "‡∏à‡∏≠‡∏á" in link_text and "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£" in link_text) else 0,
            }
            
            for exact_term, bonus in exact_match_bonuses.items():
                if bonus > 0:
                    bonus_score += bonus
                    print(f"üéØ Exact match bonus for '{exact_term}': +{bonus}")
            
            # Normalize bonus score
            normalized_bonus = min(bonus_score, 1.0)
            
            # Calculate combined score
            combined_score = (
                bm25_weight * normalized_bm25 +
                vector_weight * normalized_vector +
                bonus_weight * normalized_bonus
            )
            
            return combined_score
            
        except Exception as e:
            print(f"‚ùå Error calculating hybrid score: {e}")
            return max(bm25_score / 10.0, vector_score)  # Fallback to max of normalized scores
    
    def _advanced_thai_search(self, query: str) -> List[Document]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á exact ‡πÅ‡∏•‡∏∞ semantic matching"""
        if not PYTHAINLP_AVAILABLE:
            print("‚ö†Ô∏è PyThaiNLP not available, falling back to basic search")
            return self._exact_thai_keyword_search(query)
        
        try:
            print(f"üáπüá≠ Advanced Thai Search with PyThaiNLP: '{query}'")
            
            # 1. Normalize and tokenize query
            normalized_query = normalize(query)
            query_tokens = word_tokenize(normalized_query, engine='newmm')
            
            # 2. POS tagging to get meaningful words
            pos_tags = pos_tag(query_tokens, engine='perceptron')
            stopwords = thai_stopwords()
            
            # 3. Extract meaningful tokens (nouns, verbs, adjectives)
            meaningful_tokens = []
            for word, pos in pos_tags:
                if (pos in ['NOUN', 'VERB', 'ADJ', 'PROPN'] and 
                    word not in stopwords and 
                    len(word.strip()) > 1):
                    meaningful_tokens.append(word)
            
            # If no meaningful tokens, use original tokens
            if not meaningful_tokens:
                meaningful_tokens = [token for token in query_tokens if len(token.strip()) > 1]
            
            print(f"   üî§ Tokenized: {query_tokens}")
            print(f"   üéØ Meaningful: {meaningful_tokens}")
            
            # 4. Search in documents
            all_results = list(self._collection.find({}, limit=100))
            matched_docs = []
            
            for result in all_results:
                content = result.get("content", "")
                metadata = result.get("metadata", {})
                link_text = metadata.get("link_text", "")
                keywords = metadata.get("keywords", [])
                
                # Calculate advanced matching score
                match_score = self._calculate_thai_match_score(
                    meaningful_tokens, query_tokens, content, link_text, keywords
                )
                
                if match_score > 0:
                    doc = Document(
                        page_content=content,
                        metadata={
                            **metadata,
                            "thai_advanced_score": match_score,
                            "thai_tokens": meaningful_tokens,
                            "search_type": "thai_advanced"
                        }
                    )
                    matched_docs.append((doc, match_score))
                    print(f"  ‚úÖ Match: {link_text} (score: {match_score:.2f})")
            
            # Sort by match score
            matched_docs.sort(key=lambda x: x[1], reverse=True)
            
            result_docs = [doc for doc, score in matched_docs[:10]]
            print(f"üéØ Advanced Thai Search: Found {len(result_docs)} matches")
            
            return result_docs
            
        except Exception as e:
            print(f"‚ùå Error in advanced Thai search: {e}")
            # Fallback to basic search
            return self._exact_thai_keyword_search(query)
    
    def _calculate_thai_match_score(self, meaningful_tokens: List[str], all_tokens: List[str], 
                                   content: str, link_text: str, keywords: List[str]) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡∏≤‡∏£ match ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        score = 0.0
        
        content_lower = content.lower()
        link_text_lower = link_text.lower()
        keywords_lower = [k.lower() for k in keywords]
        
        # 1. Exact token matching in title (highest priority)
        for token in meaningful_tokens:
            if token.lower() in link_text_lower:
                score += 5.0  # High score for title match
                
        # 2. Exact token matching in content
        for token in meaningful_tokens:
            if token.lower() in content_lower:
                score += 3.0  # Medium score for content match
                
        # 3. Keyword matching
        for token in meaningful_tokens:
            for keyword in keywords_lower:
                if token.lower() in keyword or keyword in token.lower():
                    score += 2.0  # Medium score for keyword match
                    break
        
        # 4. Partial matching (for compound words)
        for token in all_tokens:
            if len(token) > 2:  # Only check longer tokens
                if token.lower() in link_text_lower:
                    score += 1.5
                elif token.lower() in content_lower:
                    score += 1.0
        
        # 5. Semantic bonus for service-related queries
        service_patterns = {
            '‡∏à‡∏≠‡∏á': ['‡∏à‡∏≠‡∏á', 'booking', 'reservation'],
            '‡∏´‡πâ‡∏≠‡∏á': ['‡∏´‡πâ‡∏≠‡∏á', 'room'],
            '‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°': ['‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', 'meeting'],
            '‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£': ['‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£', 'lab', 'laboratory'],
            '‡πÅ‡∏•‡πá‡∏ö': ['‡πÅ‡∏•‡πá‡∏ö', 'lab', 'laboratory']
        }
        
        for token in meaningful_tokens:
            if token in service_patterns:
                related_words = service_patterns[token]
                for word in related_words:
                    if word in link_text_lower or word in content_lower:
                        score += 1.0
        
        return score
    
    def _exact_thai_keyword_search(self, query: str) -> List[Document]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö exact match ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        try:
            # Thai keyword mappings for exact matching
            thai_keyword_mappings = {
                # ‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á patterns
                "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á": ["‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á", "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "reservation", "meeting"],
                "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°": ["‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á", "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "reservation", "meeting"],
                "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°": ["‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á", "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "reservation", "meeting"],
                
                # ‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö patterns  
                "‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö": ["‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£", "laboratory", "lab"],
                "‡πÅ‡∏•‡πá‡∏ö": ["‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£", "laboratory", "lab"],
                "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£": ["‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£", "laboratory", "lab"],
                "‡∏à‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö": ["‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£", "laboratory", "lab"],
                "‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö": ["‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£", "laboratory", "lab"],
            }
            
            # Get all documents from collection
            all_results = list(self._collection.find({}, limit=100))
            matched_docs = []
            
            query_lower = query.lower().strip()
            
            # Check if query matches any Thai keyword patterns
            target_keywords = thai_keyword_mappings.get(query_lower, [query_lower])
            
            print(f"üîç Thai Exact Search: '{query}' ‚Üí looking for keywords: {target_keywords}")
            
            for result in all_results:
                content = result.get("content", "").lower()
                metadata = result.get("metadata", {})
                link_text = metadata.get("link_text", "").lower()
                keywords = [k.lower() for k in metadata.get("keywords", [])]
                
                # Calculate match score
                match_score = 0.0
                matches = []
                
                # Check matches in different fields
                for keyword in target_keywords:
                    if keyword in link_text:
                        match_score += 3.0  # High score for title match
                        matches.append(f"title:{keyword}")
                    elif keyword in content:
                        match_score += 2.0  # Medium score for content match
                        matches.append(f"content:{keyword}")
                    elif any(keyword in kw for kw in keywords):
                        match_score += 1.5  # Medium score for keyword match
                        matches.append(f"keywords:{keyword}")
                
                # If we have matches, create document with score
                if match_score > 0:
                    doc = Document(
                        page_content=result.get("content", ""),
                        metadata={
                            **metadata,
                            "thai_exact_score": match_score,
                            "thai_matches": matches,
                            "search_type": "thai_exact"
                        }
                    )
                    matched_docs.append((doc, match_score))
                    print(f"  ‚úÖ Match: {metadata.get('link_text', 'Unknown')} (score: {match_score:.2f}, matches: {matches})")
            
            # Sort by match score (descending)
            matched_docs.sort(key=lambda x: x[1], reverse=True)
            
            # Return top matches
            result_docs = [doc for doc, score in matched_docs[:10]]
            print(f"üéØ Thai Exact Search: Found {len(result_docs)} matches")
            
            return result_docs
            
        except Exception as e:
            print(f"‚ùå Error in Thai exact search: {e}")
            return []

    def _fallback_keyword_search(self, query: str) -> List[Document]:
        """Fallback keyword search if BM25 fails"""
        print("üîÑ Using fallback keyword search...")
        all_documents = []
        
        try:
            keywords = self._extract_search_keywords(query)
            results = self._collection.find({}, limit=50)
            
            for result in results:
                content = result.get("content", "").lower()
                original_content = result.get("content", "")
                link_text = result.get("metadata", {}).get("link_text", "").lower()
                
                matched = False
                for keyword in keywords:
                    keyword_lower = keyword.lower()
                    
                    # Check in content or link text
                    if keyword_lower in content or keyword_lower in link_text:
                        matched = True
                        break
                    
                    # Check in keywords metadata
                    metadata_keywords = result.get("metadata", {}).get("keywords", [])
                    if any(keyword_lower in mk.lower() for mk in metadata_keywords):
                        matched = True
                        break
                
                if matched:
                    doc = Document(
                        page_content=original_content,
                        metadata=result.get("metadata", {})
                    )
                    if not any(d.page_content == doc.page_content for d in all_documents):
                        all_documents.append(doc)
            
            print(f"üìù Fallback search: ‡∏û‡∏ö {len(all_documents)} documents")
            return all_documents[:10]
            
        except Exception as e:
            print(f"‚ùå Error in fallback search: {e}")
            return []
    
    def _extract_search_keywords(self, query: str) -> List[str]:
        """‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"""
        import re
        
        # Remove common words
        stop_words = ["‡∏Ç‡∏≠", "‡∏•‡∏¥‡∏á‡∏Å‡πå", "‡∏•‡∏¥‡∏á‡∏Ñ‡πå", "‡∏´‡∏≤", "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤", "‡∏ö‡∏≠‡∏Å", "‡πÅ‡∏™‡∏î‡∏á", "‡πÉ‡∏Ñ‡∏£", "‡∏Ñ‡∏∑‡∏≠", "‡∏Ç‡∏≠‡∏á", "‡πÉ‡∏ô", "‡∏ó‡∏µ‡πà", "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠"]
        
        keywords = []
        
        # Method 1: Clean version - remove stop words first
        query_clean = query
        for stop_word in stop_words:
            query_clean = query_clean.replace(stop_word, " ")
        query_clean = re.sub(r'\s+', ' ', query_clean).strip()
        
        if query_clean and len(query_clean) >= 2:
            keywords.append(query_clean)
        
        # Method 2: Split by spaces
        words_by_space = query.split()
        for word in words_by_space:
            clean_word = word.strip()
            if clean_word not in stop_words and len(clean_word) >= 2:
                keywords.append(clean_word)
        
        # Method 3: Add specific service keywords
        service_keywords = {
            "‡∏à‡∏≠‡∏á": ["‡∏à‡∏≠‡∏á", "booking", "reservation"],
            "‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°": ["‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "meeting", "room"],
            "‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö": ["‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö", "lab", "laboratory"],
            "‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î": ["‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î", "upload"],
            "‡πÄ‡∏ü‡∏™‡∏ö‡∏∏‡πä‡∏Å": ["‡πÄ‡∏ü‡∏™‡∏ö‡∏∏‡πä‡∏Å", "facebook"]
        }
        
        for service, related_words in service_keywords.items():
            if any(word in query.lower() for word in related_words):
                keywords.extend(related_words)
        
        # Remove duplicates while preserving order
        unique_keywords = []
        for keyword in keywords:
            if keyword not in unique_keywords and len(keyword) >= 2:
                unique_keywords.append(keyword)
        
        print(f"üî§ Debug: Query '{query}' -> Keywords: {unique_keywords}")
        return unique_keywords

retriever = LinksRetriever(collection, embedding)

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏¥‡∏á‡∏Å‡πå
PROMPT = PromptTemplate.from_template("""
‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ì‡∏∞

‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÉ‡∏´‡πâ‡∏ô‡∏≥‡∏°‡∏≤‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ

‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:
- ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£: [‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£]
- ‡∏•‡∏¥‡∏á‡∏Å‡πå: [URL]
- ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: [‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£]

‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏¥‡∏á‡∏Å‡πå ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô:
- ‡πÉ‡∏ä‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÄ‡∏ä‡πà‡∏ô "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£:"
- ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà
- ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ ‚Ä¢ ‡∏´‡∏£‡∏∑‡∏≠ - ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏•‡∏¥‡∏á‡∏Å‡πå
- ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞ URL

‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏•‡∏¢‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏© ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"

---------------------
{context}
---------------------
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}
‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢):
""")

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î Chat Model - Fixed for OpenRouter
openrouter_api_key = os.getenv("OPENROUTER_API_KEY")

if not openrouter_api_key:
    print("‚ö†Ô∏è Warning: OPENROUTER_API_KEY not found in .env file")
    print("LLM responses will not work without API key")
    llm = None
else:
    try:
        llm = ChatOpenAI(
            model="openai/gpt-4o-2024-11-20",  # Free model on OpenRouter
            temperature=0,
            openai_api_key=openrouter_api_key,
            openai_api_base="https://openrouter.ai/api/v1",
            default_headers={
                "HTTP-Referer": "https://github.com/your-repo",
                "X-Title": "Links RAG Chatbot"
            }
        )
        print("‚úÖ OpenRouter LLM initialized successfully")
    except Exception as e:
        print(f"‚ùå Error initializing LLM: {e}")
        llm = None

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Manual QA Function
def manual_qa_chain(question: str) -> str:
    """
    Manual QA chain ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå - Links Version with astrapy
    """
    try:
        print(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}")
        print("üåê ‡πÉ‡∏ä‡πâ AstraDB Cloud Vector Database (astrapy) - Links Collection")
        print(f"üìö Collection: links_embedding")
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 1: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å retriever
        retrieved_docs = retriever.get_relevant_documents(question)
        
        if not retrieved_docs:
            return "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏© ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
        
        print(f"üìö ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏¥‡∏á‡∏Å‡πå {len(retrieved_docs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å AstraDB")
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 2: ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô
        print("\n" + "="*60)
        print("üìã ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å AstraDB (Links Collection):")
        print("="*60)
        
        for i, doc in enumerate(retrieved_docs, 1):
            combined_score = doc.metadata.get('combined_score', 0.0)
            bm25_score = doc.metadata.get('bm25_score', 0.0)
            vector_score = doc.metadata.get('vector_score', 0.0)
            
            print(f"\nüî∏ ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà {i}:")
            print(f"   ‡∏ä‡∏∑‡πà‡∏≠: {doc.metadata.get('link_text', 'Unknown')}")
            print(f"   URL: {doc.metadata.get('url', 'Unknown')}")
            if combined_score > 0:
                print(f"   üìä ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {combined_score:.4f} (BM25: {bm25_score:.4f}, Vector: {vector_score:.4f})")
            print("-" * 40)
        
        print("\n" + "="*60)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 3: ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ
        selected_docs = retrieved_docs  # ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        
        print(f"üéØ ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(selected_docs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
        print("="*60)
        
        # ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
        context_parts = []
        print("\nüìù CONTEXT ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM:")
        print("-" * 40)
        
        for i, doc in enumerate(selected_docs, 1):
            link_text = doc.metadata.get('link_text', 'Unknown')
            print(f"üìÑ Context {i}: {link_text}")
            context_parts.append(f"‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà {i}:\n{doc.page_content}\n")
        
        context = "\n".join(context_parts)
        print("\n" + "="*60)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt
        formatted_prompt = PROMPT.format(
            context=context,
            question=question
        )
        
        print("üí≠ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...")
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 4: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM
        if llm is None:
            return "‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ API key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM"
        
        try:
            response = llm.invoke(formatted_prompt)  # Use invoke instead of predict
            return response.content.strip()
        except Exception as llm_error:
            print(f"‚ùå LLM Error: {llm_error}")
            # Return search results directly if LLM fails
            result_text = f"‡∏û‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á {len(retrieved_docs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£:\n\n"
            for i, doc in enumerate(retrieved_docs, 1):
                link_text = doc.metadata.get('link_text', 'Unknown')
                url = doc.metadata.get('url', 'Unknown')
                result_text += f"{i}. {link_text}\n   ‡∏•‡∏¥‡∏á‡∏Å‡πå: {url}\n\n"
            return result_text
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏© ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"

# ‚úÖ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ñ‡∏≤‡∏°
if __name__ == "__main__":
    print("üîó ‡∏£‡∏∞‡∏ö‡∏ö‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏°‡∏Ç. (Links Version with astrapy)")
    print("üåê ‡πÉ‡∏ä‡πâ AstraDB Cloud Vector Database - Links Collection")
    print(f"üìö Collection: links_embedding")
    print("‡∏û‡∏¥‡∏°‡∏û‡πå 'exit' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å\n")
    print("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:")
    print("- ‡∏Ç‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏à‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°")
    print("- ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå")
    print("- ‡πÅ‡∏™‡∏î‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    print("-" * 50)

    while True:
        question = input("‚ùì ‡∏ñ‡∏≤‡∏°‡∏°‡∏≤‡πÄ‡∏•‡∏¢: ")
        if question.lower() == "exit":
            break

        # ‡πÉ‡∏ä‡πâ manual QA chain ‡πÅ‡∏ó‡∏ô
        result = manual_qa_chain(question)
        print("ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:", result)
        print("-" * 50)
