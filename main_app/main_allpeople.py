# main_allpeople.py - AstraDB Version ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö allpeople_data.py ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain_community.retrievers import BM25Retriever
from langchain.prompts import PromptTemplate
from langchain.schema import BaseRetriever, Document
from langchain.callbacks.manager import CallbackManagerForRetrieverRun
import os
from dotenv import load_dotenv
from typing import List 
from astrapy import DataAPIClient

load_dotenv()

# ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° embedding
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Initialize AstraDB client
token = os.getenv("ASTRA_DB_APPLICATION_TOKEN")
api_endpoint = os.getenv("ASTRA_DB_API_ENDPOINT")
keyspace = os.getenv("ASTRA_DB_KEYSPACE", "default_keyspace")

if not token or not api_endpoint:
    print("‚ùå Error: Missing AstraDB credentials in .env file")
    exit(1)

client = DataAPIClient(token=token)
database = client.get_database_by_api_endpoint(api_endpoint)

# Get single collection for allpeople data
try:
    collection = database.get_collection("allpeople_embedding")
    print(f"‚úÖ Connected to collection: allpeople_embedding")
except Exception as e:
    print(f"‚ùå Error accessing collection: {e}")
    exit(1)

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Custom Retriever ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AstraDB (Single Collection)
class AllPeopleRetriever(BaseRetriever):
    def __init__(self, collection, embedding):
        super().__init__()
        self._collection = collection
        self._embedding = embedding
        self._bm25_retriever = None  # Will be initialized when first used
        self._documents_cache = None  # Cache documents for BM25
    
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        print(f"üîç Debug: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ query: '{query}'")
        
        # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏û‡∏¥‡πÄ‡∏®‡∏©
        if any(word in query.lower() for word in ["‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô", "all", "15", "20", "30", "‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠"]):
            print("üéØ ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î - ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°")
            return self._get_comprehensive_search()
        
        # Try multiple search strategies
        print("üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ AstraDB hybrid search...")
        
        # Strategy 1: Text search first for exact name matches
        print("üîç ‡πÄ‡∏£‡∏¥‡πà‡∏° Text Search...")
        text_results = self._text_search(query)
        
        # Strategy 2: Vector search
        print("üß† ‡πÄ‡∏£‡∏¥‡πà‡∏° Vector Search...")
        vector_results = self._vector_search(query)
        
        # Combine results with hybrid scoring and ranking
        all_documents = []
        seen_content = set()
        
        print("üîÑ ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°...")
        
        # Collect all unique documents with their scores
        candidate_docs = []
        
        # Add text search results
        for i, doc in enumerate(text_results):
            if doc.page_content not in seen_content:
                bm25_score = doc.metadata.get("bm25_score", 0.0)
                doc.metadata["combined_score"] = self._calculate_hybrid_score(doc, bm25_score, 0.0, query)
                candidate_docs.append(doc)
                seen_content.add(doc.page_content)
                content_preview = doc.page_content[:50]
                print(f"‚ûï Text Search #{i+1}: {content_preview}... (BM25: {bm25_score:.4f})")
        
        # Add vector search results
        for i, doc in enumerate(vector_results):
            if doc.page_content not in seen_content:
                vector_score = doc.metadata.get("vector_score", 0.0)
                doc.metadata["combined_score"] = self._calculate_hybrid_score(doc, 0.0, vector_score, query)
                candidate_docs.append(doc)
                seen_content.add(doc.page_content)
                content_preview = doc.page_content[:50]
                print(f"‚ûï Vector Search #{i+1}: {content_preview}... (Vector: {vector_score:.4f})")
            else:
                # Update existing document with vector score
                for existing_doc in candidate_docs:
                    if existing_doc.page_content == doc.page_content:
                        vector_score = doc.metadata.get("vector_score", 0.0)
                        bm25_score = existing_doc.metadata.get("bm25_score", 0.0)
                        existing_doc.metadata["vector_score"] = vector_score
                        existing_doc.metadata["combined_score"] = self._calculate_hybrid_score(existing_doc, bm25_score, vector_score, query)
                        content_preview = existing_doc.page_content[:50]
                        print(f"üîÑ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {content_preview}... (BM25: {bm25_score:.4f}, Vector: {vector_score:.4f})")
                        break
        
        # Sort by combined score (descending)
        candidate_docs.sort(key=lambda doc: doc.metadata.get("combined_score", 0), reverse=True)
        
        # Take top results
        all_documents = candidate_docs
        
        # Show final ranking with scores
        print("\nüèÜ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°):")
        print("-" * 60)
        for i, doc in enumerate(all_documents[:5], 1):
            combined_score = doc.metadata.get("combined_score", 0.0)
            bm25_score = doc.metadata.get("bm25_score", 0.0)
            vector_score = doc.metadata.get("vector_score", 0.0)
            content_preview = doc.page_content[:60]
            
            print(f"#{i}: {content_preview}...")
            print(f"    üéØ Combined: {combined_score:.4f} | üìù BM25: {bm25_score:.4f} | üß† Vector: {vector_score:.4f}")
            print("-" * 40)
        
        print(f"üìä ‡∏™‡∏£‡∏∏‡∏õ: Text={len(text_results)}, Vector={len(vector_results)}, ‡∏£‡∏ß‡∏°={len(all_documents)} (unique)")
        
        return all_documents[:20]  # Return top 20 results
    
    def _get_comprehensive_search(self) -> List[Document]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å collection"""
        print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏à‡∏≤‡∏Å allpeople_embedding collection...")
        
        all_documents = []
        
        try:
            print("üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å allpeople_embedding collection...")
            results = self._collection.find({}, limit=65)  # Get up to 50 records
            
            for result in results:
                doc = Document(
                    page_content=result.get("content", ""),
                    metadata=result.get("metadata", {})
                )
                all_documents.append(doc)
            
            print(f"üìä ‡∏à‡∏≤‡∏Å allpeople_embedding: {len(all_documents)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            
        except Exception as e:
            print(f"‚ùå Error in comprehensive search: {e}")
        
        print(f"üéØ ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏£‡∏ß‡∏°: {len(all_documents)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        return all_documents
    
    def _vector_search(self, query: str) -> List[Document]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö vector search ‡∏à‡∏≤‡∏Å collection ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á"""
        all_documents = []
        
        try:
            print(f"üß† Vector Search: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö query: '{query}'")
            
            # Generate query embedding
            query_vector = self._embedding.embed_query(query)
            print(f"üìä Vector Search: ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡πÅ‡∏•‡πâ‡∏ß (dimension: {len(query_vector)})")
            
            # Perform vector search with similarity scores
            results = self._collection.find(
                {},
                sort={"$vector": query_vector},
                limit=15,  # Top 15 semantic matches
                include_similarity=True  # Include cosine similarity scores
            )
            
            for result in results:
                # Get similarity score (cosine similarity from AstraDB)
                similarity_score = result.get("$similarity", 0.0)
                
                # Create metadata with score
                metadata = result.get("metadata", {}).copy()
                metadata["vector_score"] = similarity_score
                metadata["search_type"] = "vector"
                
                doc = Document(
                    page_content=result.get("content", ""),
                    metadata=metadata
                )
                all_documents.append(doc)
            
            print(f"üß† Vector search: ‡∏û‡∏ö {len(all_documents)} documents ‡∏à‡∏≤‡∏Å semantic similarity")
            
            # Show top matches with scores
            if all_documents:
                print("   üèÜ Top Vector Matches:")
                for i, doc in enumerate(all_documents[:3]):
                    score = doc.metadata.get("vector_score", 0.0)
                    content_preview = doc.page_content[:50]
                    print(f"   #{i+1}: {content_preview}... (score: {score:.4f})")
            
            return all_documents
            
        except Exception as e:
            print(f"‚ùå Error in vector search: {e}")
            return []

    def _ensure_bm25_initialized(self):
        """Initialize BM25 retriever if not already done"""
        if self._bm25_retriever is None:
            print("üîß Initializing BM25 retriever...")
            try:
                # Get all documents from collection for BM25
                results = self._collection.find({}, limit=200)  # Increase limit for better BM25 corpus
                documents = []
                
                for result in results:
                    doc = Document(
                        page_content=result.get("content", ""),
                        metadata=result.get("metadata", {})
                    )
                    documents.append(doc)
                
                self._documents_cache = documents
                print(f"üìö Loaded {len(documents)} documents for BM25")
                
                # Create BM25 retriever
                if documents:
                    self._bm25_retriever = BM25Retriever.from_documents(documents)
                    self._bm25_retriever.k = 15  # Return top 15 results
                    print("‚úÖ BM25 retriever initialized successfully")
                else:
                    print("‚ö†Ô∏è No documents found for BM25 initialization")
                    
            except Exception as e:
                print(f"‚ùå Error initializing BM25: {e}")
                self._bm25_retriever = None

    def _preprocess_query_for_bm25(self, query: str) -> str:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ BM25 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ"""
        import re
        
        # Remove common prefixes but keep the core name
        processed = query.replace("‡∏Ç‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "").replace("‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå", "").strip()
        
        # Don't try to add spaces - just return the clean name
        # BM25 should handle Thai text better without forced spacing
        
        print(f"üî§ Query preprocessing: '{query}' ‚Üí '{processed}'")
        return processed if processed else query

    def _text_search(self, query: str) -> List[Document]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö BM25 text search ‡∏à‡∏≤‡∏Å collection ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"""
        try:
            # Ensure BM25 is initialized
            self._ensure_bm25_initialized()
            
            if self._bm25_retriever is None:
                print("‚ö†Ô∏è BM25 not available, falling back to keyword search")
                return self._fallback_keyword_search(query)
            
            print(f"üîç BM25 Search: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ query: '{query}'")
            
            # Try multiple query variations for better matching
            query_variations = [
                query,  # Original query
                self._preprocess_query_for_bm25(query),  # Preprocessed (just remove prefixes)
                query.replace("‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå", "").replace("‡∏Ç‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "").strip(),  # Clean version
            ]
            
            # Remove duplicates while preserving order
            unique_variations = []
            for q in query_variations:
                if q and q not in unique_variations:
                    unique_variations.append(q)
            
            all_bm25_results = []
            seen_content = set()
            
            for i, q_variant in enumerate(unique_variations):
                print(f"üîç BM25 Variant #{i+1}: '{q_variant}'")
                try:
                    variant_results = self._bm25_retriever.get_relevant_documents(q_variant)
                    
                    # Calculate BM25 scores for this variant
                    scored_results = self._calculate_bm25_scores(variant_results, q_variant)
                    
                    # Add unique results with scores
                    for doc, bm25_score in scored_results:
                        if doc.page_content not in seen_content:
                            # Add BM25 score to metadata
                            doc.metadata["bm25_score"] = bm25_score
                            doc.metadata["search_type"] = "text"
                            doc.metadata["query_variant"] = q_variant
                            
                            all_bm25_results.append(doc)
                            seen_content.add(doc.page_content)
                    
                    print(f"   Found {len(variant_results)} docs (unique so far: {len(all_bm25_results)})")
                    
                except Exception as e:
                    print(f"   Error with variant '{q_variant}': {e}")
            
            # Sort by BM25 score (descending)
            all_bm25_results.sort(key=lambda doc: doc.metadata.get("bm25_score", 0), reverse=True)
            
            print(f"üìù BM25 search: ‡∏û‡∏ö {len(all_bm25_results)} documents ‡∏£‡∏ß‡∏°")
            
            # Show top matches with scores
            if all_bm25_results:
                print("   üèÜ Top BM25 Matches:")
                for i, doc in enumerate(all_bm25_results[:3]):
                    score = doc.metadata.get("bm25_score", 0.0)
                    content_preview = doc.page_content[:50]
                    print(f"   #{i+1}: {content_preview}... (BM25: {score:.4f})")
            
            return all_bm25_results[:15]  # Limit to top 15
            
        except Exception as e:
            print(f"‚ùå Error in BM25 search: {e}")
            return self._fallback_keyword_search(query)
    
    def _calculate_bm25_scores(self, documents: List[Document], query: str) -> List[tuple]:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô BM25 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö documents (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå)"""
        try:
            # Simple BM25-like scoring based on term frequency and document length
            import re
            from collections import Counter
            
            query_terms = re.findall(r'\w+', query.lower())
            if not query_terms:
                return [(doc, 0.0) for doc in documents]
            
            scored_docs = []
            
            for doc in documents:
                content_lower = doc.page_content.lower()
                content_terms = re.findall(r'\w+', content_lower)
                
                if not content_terms:
                    scored_docs.append((doc, 0.0))
                    continue
                
                # Calculate term frequency
                term_freq = Counter(content_terms)
                doc_length = len(content_terms)
                
                # Simple BM25-like score
                score = 0.0
                for term in query_terms:
                    if term in term_freq:
                        tf = term_freq[term]
                        # Simplified BM25 formula (without IDF calculation)
                        score += (tf * 2.2) / (tf + 1.2 * (0.25 + 0.75 * doc_length / 50))
                
                # Boost score for exact matches in names and positions
                # Check for name matches (higher boost for faculty data)
                for term in query_terms:
                    if term in content_lower:
                        # Higher boost for name matches in faculty data
                        if any(keyword in content_lower for keyword in ["‡∏ä‡∏∑‡πà‡∏≠", "name", "‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå"]):
                            score += 3.0  # High boost for name context
                        elif any(keyword in content_lower for keyword in ["‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á", "position", "‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤"]):
                            score += 2.0  # Medium boost for position context
                        else:
                            score += 1.0  # Base boost for other matches
                
                scored_docs.append((doc, score))
            
            return scored_docs
            
        except Exception as e:
            print(f"‚ùå Error calculating BM25 scores: {e}")
            return [(doc, 0.0) for doc in documents]
    
    def _calculate_hybrid_score(self, doc: Document, bm25_score: float, vector_score: float, query: str) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å BM25 ‡πÅ‡∏•‡∏∞ Vector similarity (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå)"""
        try:
            # Weights for different components
            bm25_weight = 0.4      # 40% for text matching
            vector_weight = 0.4    # 40% for semantic similarity
            bonus_weight = 0.2     # 20% for exact matches and metadata
            
            # Base scores (normalized)
            normalized_bm25 = min(bm25_score / 10.0, 1.0)  # Normalize BM25 to 0-1
            normalized_vector = vector_score  # Vector score is already 0-1
            
            # Calculate bonus score for exact matches (faculty-specific)
            bonus_score = 0.0
            query_lower = query.lower()
            content_lower = doc.page_content.lower()
            
            # Name match bonus (high priority for faculty search)
            query_words = [word for word in query_lower.split() if len(word) >= 2]
            for word in query_words:
                if word in content_lower:
                    # Higher bonus for exact name matches
                    if any(name_indicator in content_lower for name_indicator in ["‡∏ä‡∏∑‡πà‡∏≠", "name", "‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå"]):
                        bonus_score += 0.6  # High bonus for name context
                    elif any(pos_indicator in content_lower for pos_indicator in ["‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á", "position", "‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤"]):
                        bonus_score += 0.4  # Medium bonus for position context
                    else:
                        bonus_score += 0.2  # Base bonus for other matches
            
            # Faculty-specific bonus terms
            faculty_bonuses = {
                "‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå": 0.3 if "‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå" in content_lower else 0,
                "‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢": 0.2 if "‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢" in content_lower else 0,
                "‡∏£‡∏≠‡∏á": 0.2 if "‡∏£‡∏≠‡∏á" in content_lower else 0,
                "‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå": 0.3 if "‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå" in content_lower else 0,
                "‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤": 0.2 if "‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤" in content_lower else 0,
            }
            
            for term, bonus in faculty_bonuses.items():
                if term in query_lower:
                    bonus_score += bonus
            
            # Normalize bonus score
            normalized_bonus = min(bonus_score, 1.0)
            
            # Calculate combined score
            combined_score = (
                bm25_weight * normalized_bm25 +
                vector_weight * normalized_vector +
                bonus_weight * normalized_bonus
            )
            
            return combined_score
            
        except Exception as e:
            print(f"‚ùå Error calculating hybrid score: {e}")
            return max(bm25_score / 10.0, vector_score)  # Fallback to max of normalized scores
    
    def _fallback_keyword_search(self, query: str) -> List[Document]:
        """Fallback keyword search if BM25 fails - with better Thai name matching"""
        print("üîÑ Using fallback keyword search...")
        all_documents = []
        
        try:
            keywords = self._extract_search_keywords(query)
            results = self._collection.find({}, limit=100)
            
            for result in results:
                content = result.get("content", "").lower()
                original_content = result.get("content", "")
                
                matched = False
                for keyword in keywords:
                    keyword_lower = keyword.lower()
                    
                    # Method 1: Direct substring match
                    if keyword_lower in content:
                        matched = True
                        break
                    
                    # Method 2: Character-by-character fuzzy match for Thai names
                    elif len(keyword) >= 3:
                        # Remove spaces from both content and keyword for comparison
                        content_no_space = content.replace(" ", "")
                        keyword_no_space = keyword_lower.replace(" ", "")
                        
                        if keyword_no_space in content_no_space:
                            matched = True
                            print(f"üéØ Fuzzy match: '{keyword}' found in content (no spaces)")
                            break
                        
                        # Method 3: Try partial matches for compound names
                        if len(keyword) > 5:
                            # Split keyword into parts and check if all parts exist
                            keyword_parts = [p for p in keyword_lower.split() if len(p) >= 2]
                            if len(keyword_parts) >= 2:
                                if all(part in content for part in keyword_parts):
                                    matched = True
                                    print(f"üéØ Partial match: All parts of '{keyword}' found")
                                    break
                
                if matched:
                    doc = Document(
                        page_content=original_content,
                        metadata=result.get("metadata", {})
                    )
                    if not any(d.page_content == doc.page_content for d in all_documents):
                        all_documents.append(doc)
            
            print(f"üìù Fallback search: ‡∏û‡∏ö {len(all_documents)} documents")
            return all_documents[:15]
            
        except Exception as e:
            print(f"‚ùå Error in fallback search: {e}")
            return []
    
    def _extract_search_keywords(self, query: str) -> List[str]:
        """‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô"""
        import re
        
        # Remove common words
        stop_words = ["‡∏Ç‡∏≠", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå", "‡∏´‡∏≤", "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤", "‡∏ö‡∏≠‡∏Å", "‡πÅ‡∏™‡∏î‡∏á", "‡πÉ‡∏Ñ‡∏£", "‡∏Ñ‡∏∑‡∏≠", "‡∏Ç‡∏≠‡∏á", "‡πÉ‡∏ô", "‡∏ó‡∏µ‡πà", "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠"]
        
        keywords = []
        
        # Method 1: Clean version - remove stop words first
        query_clean = query
        for stop_word in stop_words:
            query_clean = query_clean.replace(stop_word, " ")
        query_clean = re.sub(r'\s+', ' ', query_clean).strip()
        
        if query_clean and len(query_clean) >= 2:
            keywords.append(query_clean)
        
        # Method 2: Split by spaces (for queries with spaces)
        words_by_space = query.split()
        for word in words_by_space:
            clean_word = word.strip()
            if clean_word not in stop_words and len(clean_word) >= 2:
                keywords.append(clean_word)
        
        # Method 3: Extract Thai name patterns (but don't break them up)
        # Look for longer Thai sequences that might be names
        thai_name_pattern = r'[‡∏Å-‡πô]{3,15}'  # 3-15 characters for names
        potential_names = re.findall(thai_name_pattern, query)
        for name in potential_names:
            if name not in stop_words and len(name) >= 3 and name not in keywords:
                keywords.append(name)
        
        # Method 4: Add parts after removing stop words
        clean_parts = [part.strip() for part in query_clean.split() if part.strip()]
        for part in clean_parts:
            if len(part) >= 2 and part not in keywords:
                keywords.append(part)
        
        # Method 5: Add the original query as fallback
        if query.strip() not in keywords:
            keywords.append(query.strip())
        
        # Remove duplicates while preserving order
        unique_keywords = []
        for keyword in keywords:
            if keyword not in unique_keywords and len(keyword) >= 2:
                unique_keywords.append(keyword)
        
        print(f"üî§ Debug: Query '{query}' -> Keywords: {unique_keywords}")
        return unique_keywords

retriever = AllPeopleRetriever(collection, embedding)

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå
PROMPT = PromptTemplate.from_template("""
‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÉ‡∏ô‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÉ‡∏ô‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô 

‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÉ‡∏´‡πâ‡∏ô‡∏≥‡∏°‡∏≤‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡∏≠‡∏¢‡πà‡∏≤‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡πâ‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà

‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
- ‡πÉ‡∏ä‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÄ‡∏ä‡πà‡∏ô "‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÉ‡∏ô‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå:"
- ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà
- ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ ‚Ä¢ ‡∏´‡∏£‡∏∑‡∏≠ - ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô
- ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠ ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‡∏≠‡∏µ‡πÄ‡∏°‡∏•

‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏Ñ‡∏ô‡πÉ‡∏î‡∏Ñ‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô:
- ‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• (‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
- ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£
- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠ (‡∏≠‡∏µ‡πÄ‡∏°‡∏•, ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£)
- Slug (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)

‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏•‡∏¢‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏© ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"

---------------------
{context}
---------------------
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}
‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢):
""")

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î Chat Model - Fixed for OpenRouter
openrouter_api_key = os.getenv("OPENROUTER_API_KEY")

if not openrouter_api_key:
    print("‚ö†Ô∏è Warning: OPENROUTER_API_KEY not found in .env file")
    print("LLM responses will not work without API key")
    llm = None
else:
    try:
        llm = ChatOpenAI(
            model="openai/gpt-4o-2024-11-20",  # Free model on OpenRouter
            temperature=0,
            openai_api_key=openrouter_api_key,
            openai_api_base="https://openrouter.ai/api/v1",
            default_headers={
                "HTTP-Referer": "https://github.com/your-repo",
                "X-Title": "Faculty RAG Chatbot"
            }
        )
        print("‚úÖ OpenRouter LLM initialized successfully")
    except Exception as e:
        print(f"‚ùå Error initializing LLM: {e}")
        llm = None

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Manual QA Function
def manual_qa_chain(question: str) -> str:
    """
    Manual QA chain ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô - AllPeople Version with astrapy
    """
    try:
        print(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}")
        print("üåê ‡πÉ‡∏ä‡πâ AstraDB Cloud Vector Database (astrapy) - AllPeople Collection")
        print(f"üìö Collection: allpeople_embedding")
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 1: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å retriever
        retrieved_docs = retriever.get_relevant_documents(question)
        
        if not retrieved_docs:
            return "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏© ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
        
        print(f"üìö ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(retrieved_docs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å AstraDB")
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 2: ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô
        print("\n" + "="*60)
        print("üìã ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å AstraDB (AllPeople Collection):")
        print("="*60)
        
        for i, doc in enumerate(retrieved_docs, 1):
            combined_score = doc.metadata.get('combined_score', 0.0)
            bm25_score = doc.metadata.get('bm25_score', 0.0)
            vector_score = doc.metadata.get('vector_score', 0.0)
            
            print(f"\nüî∏ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà {i}:")
            if combined_score > 0:
                print(f"   üìä ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {combined_score:.4f} (BM25: {bm25_score:.4f}, Vector: {vector_score:.4f})")
            print("-" * 40)
            print(doc.page_content.strip())
            print("-" * 40)
        
        print("\n" + "="*60)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 3: ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ
        selected_docs = retrieved_docs  # ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        
        print(f"üéØ ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(selected_docs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
        print(f"üí° ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î")
        print("="*60)
        
        # ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
        context_parts = []
        print("\nüìù CONTEXT ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM:")
        print("-" * 40)
        
        for i, doc in enumerate(selected_docs, 1):
            print(f"üìÑ Context {i}: {doc.page_content.strip()[:100]}...")
            context_parts.append(f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà {i}:\n{doc.page_content}\n")
        
        context = "\n".join(context_parts)
        print("\n" + "="*60)
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt
        formatted_prompt = PROMPT.format(
            context=context,
            question=question
        )
        
        print("üí≠ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö...")
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 4: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM
        if llm is None:
            return "‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ API key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM"
        
        try:
            response = llm.invoke(formatted_prompt)  # Use invoke instead of predict
            return response.content.strip()
        except Exception as llm_error:
            print(f"‚ùå LLM Error: {llm_error}")
            # Return search results directly if LLM fails
            return f"‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á {len(retrieved_docs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ"
        
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏© ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"

# ‚úÖ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ñ‡∏≤‡∏°
if __name__ == "__main__":
    print("üéì ‡∏£‡∏∞‡∏ö‡∏ö‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ì‡∏∞‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡∏°‡∏Ç. (AllPeople Version with astrapy)")
    print("üåê ‡πÉ‡∏ä‡πâ AstraDB Cloud Vector Database - AllPeople Collection")
    print(f"üìö Collection: allpeople_embedding")
    print("‡∏û‡∏¥‡∏°‡∏û‡πå 'exit' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å\n")

    while True:
        question = input("‚ùì ‡∏ñ‡∏≤‡∏°‡∏°‡∏≤‡πÄ‡∏•‡∏¢: ")
        if question.lower() == "exit":
            break

        # ‡πÉ‡∏ä‡πâ manual QA chain ‡πÅ‡∏ó‡∏ô
        result = manual_qa_chain(question)
        print("ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:", result)
        print("-" * 50)
